{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Purpose</h2>\n",
    "<ul>\n",
    "<li>Reviewing why Fully Connected Neural networks are not good choices when analying imaging data</li>\n",
    "<li>Understanding convolution and why convolutional neural networks are better choice when creating models for imaging data </li>\n",
    "<li>Creating a CNN</li>\n",
    "<li>Understanding the difference between the module and the functional APIs</li>\n",
    "<li>Designing a neural network</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Why FCNs are not good choices for analyzing images?</h3>\n",
    "<ul>\n",
    "    <li>FCNs have too many parameters. As such, they can memorize the details of the training set, and therefore the the resulted models, most probably, will not be generalized well.</li>\n",
    "    <li>Another issue with having too many parameters is that such a network is computationally expensive to train. </li>\n",
    "    <li>Every pixel is related to all other pixels regardless of their locations. Therefore, FCNs are not able to capture local patterns</li>\n",
    "</ul>\n",
    "<p><span style='color:green;'>Alternative solution:</span> Using <span style='color:red;'>convolution</span>, which is a different linear operation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How CNN tackles the above issues?</h3>\n",
    "<p><b>Learning more with less number of parameters:</b> Kernels are one of the main building blocks of the CNNs Kernels are small matrices; therefore, CNNs are able to learn features with smaller numbers of parameters. </p>\n",
    "<p><b>Downsampling:</b> CNNs use downsampling methods, such as maxpooling or average pooling, to keep and send the most informative part of the image to the next layer through less number of parameters. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolution</h3>\n",
    "<p>The main building blocks of CNNs are <span style='background-color:yellow'><b>kernels</b></span>. In image processing, a kernel is a 2D (or 3D in the case of colored images) signal or a matrix which is used for different purposes, such as sharpening, blurring, edge detection. In a CNN model a kernel enables to capture similar patterns across an image regardless of their locations. This property is called 'translation invariant' by the book. To explain more, a kernel slides over an image and the elements of the kernel (remember that a kernel is a matrix) are convolved with the pixels of the image. Thus, one of the main operations in a CNN is convolution and hence the name of these types of neural networks.     \n",
    "<br/>\n",
    "    </p>\n",
    "    <p>\n",
    "<b>Notes</b>\n",
    "    <ul>\n",
    "        <li>In the context of CNNs, a kernel is called <span style='background-color:yellow'><b>Channel</b></span> too. An RGB colored image has three channels, red, green, and blue, meaning there are three values associated with every pixels. Similarly, a CNN has several channels at every convolutional layer. Each of these channels or kernels are responsible to learn a specific pattern.</li>\n",
    "        <li>At every layer, a CNN has several kernels and hence is able to capture more than one pattern.</li>\n",
    "        <li>Although there are several kernels at every layer, the size of the kernels is small (e.g., 3*3 or 5*5). Thus, a CNN model can have a much smaller set of parameters in comparison with an FCN, but still be able to reach better results.</li>\n",
    "        <li>Having several layers each of which consisting several kernels can be interpreted as transforming an image with multiple channels to another one again with multiple channels.</li>\n",
    "        <li><i><b>torch.nn</b></i> modules provides several modules to perform convolution; <i><b>nn.Conv1d</b></i> for timeseries or 1D signals, <i><b>nn.Conv2d</b></i> for images or <i><b>2D signals</b></i>, and <i><b>nn.Conv3d</b></i> for volums and videos or 3D signals</li>\n",
    "    </ul>\n",
    "    </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Downsampling</h3>\n",
    "<ul>\n",
    "    <li><b>Averaging pooling:</b> Averaging pixels within a specified window to create one pixel value results in reducing the size of an image. To illustrate, performin average pooling within a window of size 2*2 means we take average over the four pixels within this window. Every four pixels are mapped to one pixel value. Therefore, an image of size 4*4 with 16 pixels is transformed to a 2*2 image with 4 pixels. </li>\n",
    "    <li><b>Maxpooling:</b> This operation is similar to average pooling but rather than taking average, the maximum pixel value will be selected within the specified window.</li>\n",
    "    <li><b>Striding:</b> If we use steps with size bigger than one when sliding a kernel over an image, the result will be an image with the size smaller that the original image.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deeper layers generate more complx features</h3>\n",
    "<p>\n",
    " At every layer, the network tries to compact information within less parameters and send it over deeper layers. As a result we can say that kernels in the first layers operates over small neighboring pixels while kernels deeper within the network work are looking at the compacted information generated by previous layers. Therefore, the deeper layers can capture more complex patters and features by looking a wider range of pixels than the kernels in the first layers.\n",
    "</p>\n",
    "<p>\n",
    "<b>Receptive field:</b> The number of input pixels that a layer uses to produce one output pixel is called receptive field.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building a CNN </h2>\n",
    "<p>In the rest of the article, we will see how to build a CNN with two approaches; 1) using nn.Sequential module and 2) as a subclass of nn.Module.  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building a CNN using nn.Sequential module</h3>\n",
    "<p>Puttng together all the concepts above, a CNN is a <span style='background-color:yellow'><i>sequence</i></span> of layers consisting several <i><b>kernels</b></i> to perform <i><b>convolution</b></i>, activation functions, and pooling operations. Let's create a simple model. Before that, we need toprepare the dataset. Similar to Chapter 7, we will work on CIFAR2 data which we creat from CIFAR10 by including only two categories of images and excluding the rest.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cifar_mean = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "cifar_std = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "#transform = transforms.compose([transforms.ToTensor(), transforms.Normalize(mean = cifar_mean, std = cifar_std)])\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=cifar_mean, std=cifar_std)])\n",
    "cifar10 = CIFAR10('/Users/lidasafarnejad/PycharmProjects/pytorchBook/data', train=True, download=False, transform = transform)\n",
    "cifar10_val = CIFAR10('/Users/lidasafarnejad/PycharmProjects/pytorchBook/data', train=False, download = False, transform=transform)\n",
    "label_map = {0:1, 2:1}\n",
    "class_names = ['aiplane', 'bird']\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in label_map]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in label_map]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "sample_img, sample_label = cifar2[0]\n",
    "print(sample_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here is the model:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Size of the image after the first convolution layer: (32-3+2)/1 + 1= 32\n",
    "# Size of the image after the first maxpooling operation: 32 / 2 = 16\n",
    "# Size of the image after the second convolution layer: (16-3+2)/1 + 1 = 16\n",
    "# Size of the image after the second maxpooling operation: 16 / 2 = 8\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,16,kernel_size=(3,3), stride=1, padding=1), \n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16,8,kernel_size=(3,3),stride=1, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(8*8*8, 2)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'><b>Notes:</b></span> \n",
    "<ul>\n",
    "    <li>Size of the output of a convolutional layer with the kernel size $k$ and stride $s$ is calculate using  the following formula:\n",
    "        $$\\frac{(w-k+2*p)}{s},$$\n",
    "        where $w$ is the width of the image and $p$ is the size of the padding. Here we assume that height and width of the image are the same. If not, to calculate the height after applying convolution, $w$ must be substitude via $h$ (height) of the image.</li>\n",
    "    <li>In the above simple model, pay attention to <i><b>nn.Flatten()</b></i> coming after the last maxpooling operation and before the linear layer. The output of the last maxpooling layer is still an image or a 2D array that cannot be consumed by the next LINEAR layer which expects 1D input. Hence, we must use nn.Flatten() in between.</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0766,  0.0352]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building a CNN as a subclass of nn.Module</h3>\n",
    "<p>While nn.Sequential is a useful tool for building CNNs, there are some scenarios where we need more flexibility to create custom network architectures. In these cases, using a predefined class like nn.Sequential, which is a subclass of nn.Module, may not provide the necessary degree of flexibility. In such scenarios we can build our own class as a subclass of nn.Module.</p>\n",
    "<p> nn.Module is an <i><b>abstract</b></i> class meaning it cannot be instantiated but it provides some useful concrete methods and also some abstract methods. A subclass of an abstract class must implement the abstract methods of its parent abstract class. Please note that when you define an abstract class, you specify a set of methods that must be implemented by any concrete subclass of the abstract class. However, a concrete subclass is free to add additional methods beyond those specified in the abstract class, as long as it implements all of the abstract methods as required. Interested readers can visit <a href='https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py'>this link</a> for the implementation of the nn.Module.</p>\n",
    "<p>Now that know what nn.Module is, let's use it to define a CNN class: </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=(3,3), padding=1)\n",
    "        self.fc1 = nn.Linear(16*8*8,32)\n",
    "        self.fc2 = nn.Linear(32,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)),2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n",
    "        out = out.view(-1,16*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "model = CNN_net()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the above example, we define our CNN network as a subclass of nn.Module, and then create an object of this class to represent our model. </p>\n",
    "<p><b><span style='color:red;'>Note:</span></b>When using nn.Module to define your network, you have to implement the <i><b>forward</b></i> method. This method is the implementation of a forward pass in your network.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The functional API</h3>\n",
    "<p>\n",
    "        <i><b>nn.Module</b></i> is an abstract class and other modules such as <i><b>nn.Linear</b></i> and <i><b>nn.Sequential</b></i> are subclasses of it. We also saw that a customized classes that we designed are also subclasses of <i><b>nn.Module</b></i>. Therefore, they can be considered to be modules. A module, as the way defined in the book, can have a state and a set of parameters. Keeping this in mind, when performing some operations such as <i><b>nn.MaxPool2d</b></i>, we don't really need to keep track of their states or even store any parameters associated with them. As such, considering them as modules is somehow overkill and hence the pytorch functional API comes to the scene. As stated in the book, '<span style='background-color:yellow;'><i>functional</i> here means 'having no state.</span> Thus, the rule of thumb is that when we want to simply perfom an operation, use the methods (functions) of <b><i>nn.functional</i></b>, such as <i><b>nn.functional.max_pool2d</b></i> rather than <i><b>nn.MaxPool2d</b></i>\n",
    "    <br/>\n",
    "    However, when keeping track of the state and parameters is important, we should use modules of <b><i>nn</i></b> rather than the functional API. As an example, the recommendation is to use <i><b>nn.Linear</b></i> rather than its counterpart <b><i>nn.functional.linear</i></b>.\n",
    "</p>\n",
    "<p><b><span style='color:red;'>Note:</span></b> Some functional tools, such as <b><i>nn.functional.tanh</i></b>, have been depricated and the recommendation is not using them eventhough they are more like a function rather than a module. Interested readers can read <a url='https://discuss.pytorch.org/t/torch-tanh-vs-torch-nn-functional-tanh/15897'>this thread</a> in the Pytorch forum.</p>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the implementation of the abstract method in the concrete class\n"
     ]
    }
   ],
   "source": [
    "class MyAbstractClass:\n",
    "    def my_abstract_method(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "class MyConcreteClass(MyAbstractClass):\n",
    "    def my_abstract_method(self):\n",
    "        print(\"This is the implementation of the abstract method in the concrete class\")\n",
    "\n",
    "my_object = MyConcreteClass()\n",
    "my_object.my_abstract_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>More to read:</h3>\n",
    "    <a>https://discuss.pytorch.org/t/torch-tanh-vs-torch-nn-functional-tanh/15897</a>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seqvenv",
   "language": "python",
   "name": "seq2seqvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
