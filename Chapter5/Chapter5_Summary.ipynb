{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refernce to these code is: https://github.com/deep-learning-with-pytorch/dlwpt-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Chapter 5: The mechanics of learning:</h2>\n",
    "    <ul>\n",
    "        <li>Learning is fitting data or making algorithm to learn from data </li>\n",
    "        <li>Fitting model involves a function with number of unknown parameters whose values are estimated from data. \n",
    "    That function in short is called <b> model </b> </li>\n",
    "        <li>Simple model in this chapter is defined as a model with bias and weight. \n",
    "        To optimize weights, a change in weight (gradient of error with respect to the parameters) is \n",
    "        computed using chain rule for derivative of composite function (backward pass) </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Overview:</h2>\n",
    "<ul>\n",
    "    <li> This chapter starts with explanation about how a model learns.</li>\n",
    "    <li> Explains that learning is actually just a parameter estimation which happens using differentiation and gradianet descent. </li>\n",
    "    <li> Simple learning algorithm which is called linear model, is explained. </li>\n",
    "    <li> Pytorch supports learning with autograd </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset:</h3>\n",
    "<ul>\n",
    "    <li> 11 datapoints are gathered by making note of temperature data in old celcius (t_c). \n",
    "    And measurement from new thermometer (t_u) </li>\n",
    "    <li><span style=\"color:Tomato;\"><b>Note:</b></span> Simple linear model is used here.</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Learning is just a parameter estimation ###\n",
    "\n",
    "In order to optimize the parameter of the model—its weights—the change in\n",
    "the error following a unit change in weights (that is, the gradient of the error with\n",
    "respect to the parameters) is computed using the chain rule for the derivative of a\n",
    "composite function (backward pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The steps to Fit the model to the input training instances are followed here: ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by making a note of temperature data in good old Celsius and measurements(inputs) from our new thermometer (target), \n",
    "and figure things out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_c are values in celsius\n",
    "# t_u are values in unknown unit\n",
    "\n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using simple model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Any model needs weight and bias. Weight and bias are two very common terms for linear\n",
    "scaling and the additive constant and will be used a lot. Here we defined weights to be 1 and bias 0 values. \n",
    "We are looking for making a model that makes <span style='background-color:#FFFF00;'> less loss </span>. \n",
    "Considering that here we have linear model, the loss as a function of w and b is also convex. \n",
    "In less complex model we are able to find global minima as explaind in this <a href='https://www.youtube.com/watch?v=JLk0ee_zuOM'>link</a>, \n",
    "however on more complex models such as neural network models finding gobal minima is not feasible \n",
    "and we end up finding optimal local minima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions are : tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
      "        48.4000, 60.4000, 68.4000])\n",
      "loss is : tensor(1763.8848)\n"
     ]
    }
   ],
   "source": [
    "def model(t_u,w,b):\n",
    "    return w*t_u+b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "w=torch.ones(())\n",
    "b=torch.zeros(())\n",
    "\n",
    "t_p=model(t_u,w,b)\n",
    "\n",
    "print(\"predictions are :\",t_p)\n",
    "\n",
    "loss=loss_fn(t_p,t_c)\n",
    "print(\"loss is :\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building a tensor of differences, taking their square element-wise,\n",
    "and finally producing a scalar loss function by averaging all of the elements in the resulting tensor. \n",
    "It is a <span style='background-color:#FFFF00;'> mean square loss </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model with Gradient descend loss and epochs ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iteratively apply updates to parameters for a fixed number of iterations,or until w and b stop changing. \n",
    "Specfiically here, we use stochastic gradient descent to update parameters for a fixed number of epochs(iterations).\n",
    "In this <a href= 'https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/'>link</a> the difference between sample, \n",
    "batch and epoch is explained. In short, The batch size is a number of samples processed before the model is updated. \n",
    "The number of epochs is the number of complete passes through the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta=0.1\n",
    "\n",
    "loss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) -loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n",
    "learning_rate=1e-2\n",
    "w = w - learning_rate * loss_rate_of_change_w\n",
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "b = b - learning_rate * loss_rate_of_change_b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2 * (t_p - t_c) / t_p.size(0)\n",
    "    return dsq_diffs\n",
    "\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def dmodel_dw(t_u,w,b):\n",
    "    return t_u\n",
    "\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0\n",
    "\n",
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n",
    "\n",
    "def training_loop(n_epochs, learning_rate, params,t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "        params = params - learning_rate * grad\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model on Normalized inputs ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train model with raw data, the gradients of weight and bias would be very large as they will be on different scales. \n",
    "As a result, learning rate that is large enough to meaningfully update one will be so large as to be unstable for the other.\n",
    "In order to reduce the range of input, we need to <span style='background-color:#FFFF00;'> normalize data </span>. \n",
    "Normaliization is an effective and easy way to improve model convergence. \n",
    "Model converges mean that the loss of learning goes to zero and data points exactly sit on line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "Epoch 2, Loss 79.752480\n",
      "Epoch 3, Loss 79.148026\n",
      "Epoch 4, Loss 78.550865\n",
      "Epoch 5, Loss 77.960899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0383, 0.0052])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The learning rate is one of the things we typically change when training does not go as well as we would like.\n",
    "\n",
    "t_un = 0.1 * t_u\n",
    "\n",
    "training_loop (\n",
    "              n_epochs=5,\n",
    "              learning_rate=1e-4,\n",
    "              params=torch.tensor([1.0,0.0]),\n",
    "              t_u=t_un,t_c=t_c\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "Epoch 2, Loss 37.574913\n",
      "Epoch 3, Loss 30.871077\n",
      "Epoch 4, Loss 29.756193\n",
      "Epoch 5, Loss 29.507153\n",
      "Epoch 6, Loss 29.392456\n",
      "Epoch 7, Loss 29.298828\n",
      "Epoch 8, Loss 29.208717\n",
      "Epoch 9, Loss 29.119415\n",
      "Epoch 10, Loss 29.030489\n"
     ]
    }
   ],
   "source": [
    "#Let’s run the loop for enough iterations to see the changes in params get small.\n",
    "\n",
    "params =  training_loop (\n",
    "                        n_epochs = 10,\n",
    "                        learning_rate = 1e-2,\n",
    "                        params = torch.tensor([1.0, 0.0]),\n",
    "                        t_u = t_un,\n",
    "                        t_c = t_c,\n",
    "                        );\n",
    "\n",
    "\n",
    "t_p=model(t_un,*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We trained on unknown units and made prediction. Circles are our input data (celcius) and line is our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0aab5c278>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fit=plt.figure(dpi=100)\n",
    "plt.xlabel(\"temperature (farenheight)\")\n",
    "plt.ylabel(\"termperature (celccius)\")\n",
    "plt.plot(t_u.numpy(),t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(),t_c.numpy(),'o' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Autograd ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors can remember where they come from, in terms of the operations and\n",
    "parent tensors that originated them, and they can automatically provide the chain of\n",
    "derivatives of such operations with respect to their inputs. As a result, \n",
    "PyTorch will automatically provide the gradient of that expression with respect to its\n",
    "input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "            \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using requires_grad=True, the grad attribute of params contains the derivatives of the loss with respect to each element of params.\n",
    "\n",
    "params= training_loop (\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c\n",
    "    );\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch abstracts the optimization strategy away from user code:\n",
    "that is, the training loop we’ve examined. This saves us from the boilerplate busywork\n",
    "of having to update each and every parameter to our model ourselves. The torch\n",
    "module has an optim submodule where we can find classes implementing different\n",
    "optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is one of the optimziers. It stands for stochastic gradient descent. Actually, the optimizer itself is exactly a\n",
    "vanilla gradient descent (as long as the momentum argument is set to 0.0, which is the\n",
    "default). The term stochastic comes from the fact that the gradient is typically obtained\n",
    "by averaging over a random subset of all input samples, called a minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train,val, test set ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A highly adaptable model will tend to use its many parameters to make\n",
    "sure the loss is minimal at the data points, it is called overfitting. We will have no guarantee that the model\n",
    "behaves well away from or in between the data points. \n",
    "The first action we can take to combat overfitting is recognizing that it might happen.\n",
    "In order to do so, we must take a few data points\n",
    "out of our dataset (the validation set) and only fit our model on the remaining data\n",
    "points (the training set). \n",
    "\n",
    "<span style='background-color:#FFFF00;'> There are two rules to detect overfitting of model: </span>\n",
    "<ul>\n",
    "<li> 1. If training loss is not decreasing, the model is overfitting.</li>\n",
    "<li> 2. In model learning, if the training loss and the validation loss diverge, we’re overfitting.</li>\n",
    "</ul>\n",
    "\n",
    "<span style='background-color:#FFFF00;'> Cures for overfitting of model: </span>\n",
    "<ul>\n",
    "<li> Add penalization term to loss function to make it cheaper for model to behave more smoothly and change more slowly</li>\n",
    "<li> Add noise to the input samples, to artificially create new data\n",
    "points in between training data samples and force the model to try to fit those, too. </li>\n",
    "<li> Increase the size of nn until it fits and then scale it down until it stops overfitting. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u\n",
    "\n",
    "\n",
    "def training_loop(n_epochs, optimizer,params, train_t_u,val_t_u,\n",
    "                  train_t_c,val_t_c):\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        train_t_p=model(train_t_u,*params)\n",
    "        train_loss=loss_fn(train_t_p,train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p=model(val_t_u,*params)\n",
    "            val_loss=loss_fn(val_t_p,val_t_c)\n",
    "            assert val_loss.requires_grad==False\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        if epoch<=3 or epoch % 500==0:\n",
    "            print(f\"epoch {epoch},Training loss  {train_loss.item():.4f}\"\n",
    "            ,\n",
    "                 f\" validation loss {val_loss.item():.4f}\")\n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,Training loss  90.7754  validation loss 33.5146\n",
      "epoch 2,Training loss  33.7982  validation loss 34.4394\n",
      "epoch 3,Training loss  27.0283  validation loss 42.0913\n",
      "epoch 500,Training loss  9.4832  validation loss 10.2596\n",
      "epoch 1000,Training loss  4.6468  validation loss 2.7845\n",
      "epoch 1500,Training loss  3.2565  validation loss 2.4603\n",
      "epoch 2000,Training loss  2.8569  validation loss 3.3453\n",
      "epoch 2500,Training loss  2.7420  validation loss 4.1242\n",
      "epoch 3000,Training loss  2.7090  validation loss 4.6294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.5929, -18.4719], requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop (\n",
    "            n_epochs = 3000,\n",
    "            optimizer = optimizer,\n",
    "            params = params,\n",
    "            train_t_u = train_t_un,\n",
    "            val_t_u = val_t_un,\n",
    "            train_t_c = train_t_c,\n",
    "            val_t_c = val_t_c\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Review of Chapter 5 </h2>\n",
    "<ul>\n",
    "<li>This chapter covers the mechanics of training neural networks using backpropagation and stochastic gradient descent.</li>\n",
    "<li> The chapter starts by introducing the concept of loss functions, which measure the difference between the predicted output and the ground truth. \n",
    "The authors explain different types of loss functions and how to choose the appropriate one for a given task.</li>\n",
    "<li> The authors then introduce the backpropagation algorithm, which is used to compute the gradients of the loss function with respect to the parameters of the model. \n",
    "They explain how to implement backpropagation in PyTorch using automatic differentiation.</li>\n",
    "<li> Next, the chapter covers the concept of optimization algorithms, which are used to update the model parameters based on the computed gradients. \n",
    "The authors explain different optimization algorithms, such as stochastic gradient descent, Adam, and Adagrad, and provide examples of how to use them in PyTorch. </li>\n",
    "<li> The chapter then explains the concept of learning rate, which determines the step size in the parameter update process. \n",
    "The authors discuss different strategies for setting the learning rate, such as using a fixed learning rate, using a learning rate schedule, or using adaptive learning rates.</li>\n",
    "<li> Finally, the chapter covers the concept of regularization, which is used to prevent overfitting in neural networks. \n",
    "The authors explain different regularization techniques, such as L1 and L2 regularization, dropout, and early stopping.</li>\n",
    "</ul> \n",
    "Overall, Chapter 5 of \"Deep Learning with PyTorch\" provides a comprehensive introduction to the mechanics of training neural networks, making it a valuable resource for anyone interested in understanding the basics of deep learning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
